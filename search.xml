<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Notes on Lasso, LARS and Stagewise Regression]]></title>
    <url>%2F2019%2F09%2F30%2FLasso-LARS-Stagewise%2F</url>
    <content type="text"><![CDATA[1. Bayesian View for Lasso and Ridge \beta = argmax(In(\Pi_{i}p(y_i|x,\beta)p(\beta))1.1 Prior Distribution of $\beta$ Normal Distribution: Ridge Laplace Distribution: Lasso p(\beta) = \frac{1}{2\alpha}exp(-\frac{|\beta|}{\alpha})2. Forward Selection, Stagewise Regression &amp; LS Boosting2.1 Forward Selection Repeat Select feature that minimize the residuals based on previous selected features 2.2 Forward Stagewise Regression1. Algorithm Step 1: Set $r = y, \beta_j = 0,1\leq j\leq d$ Step 2: Select a feature $\beta_k$ with the highest correlation with r Step 3: $\beta_k = \beta_k + \delta sign(f_k^Tr)$ Step 4: Residual $r = r - \deltasign(f_k^Tr)f_k$ Step 5: Repeat 2-4 2. Properties Less Greedy Need to set Step size and iteration rounds Large Step Size leads to OLS 2.3 LS Boosting1. Algorithm Step 1: Set $r = y, F(x) = 0$ Step 2: fit a weak model $f(x)$ on r Step 3: $F(x) = F(x) + \delta f(x)$ Step 4: Residual $r = r - \delta f(x)$ Step 5: Repeat 2-4 2.4 LARS1. Algorithm Step 1: Start with $r = y,\beta_{i=1,\cdots,p} = 0$ Assume standardized X Step 2: Find predictor $x_j$ most correlated with r Step 3: Move $\beta_j$towards its LS coefficient $sign(corr(r,x_j))$ until some other competitor $x_k$ has as much correlation with current residual as does $x_j$ Angle Bisector Direction Step 4: Move ($\beta_j,\beta_k$) in the joint LS direction for ($x_j,x_k$) and Repeat 3 If $r_k = y-X_{A_k}\beta_{A_k}$ is the current residual, then the direction for this step is \delta_k=(X_{A_k}^TX_{A_k})^{-1}X_{A_k}^Tr_k \\ \beta_{A_k} = \beta_{A_k} + \alpha \delta_k Step5: Continue in this way until all predictors have been entered. 2. Properties LARS only enters “as much” of a predictor as it deserves. 3. Three Solutions to Lasso3.1 Close-Form Solution \beta_j^{Lasso} = sign(\beta_j^{OLS})(|\beta_j^{OLS}-\frac{n\lambda}{2}|)3.2 LARS: LASSO Modification1. Modify Step 4 in LARS Step 4b: If a non-zero coefficient hits zero, drop its variable from the active set and recompute the current joint LS direction 3.3 Coordinate Descent1. Algorithm Iteration of Single Variate LASSO Step 1: Initialize $\beta$ Step 2: For k = 1:p Min loss L($\beta_j|\beta_{-j}$) L(\beta_j|\beta_{-j}) = \frac{1}{n}\sum_{i}(y_i-\sum_{s\neq k}X_{is}\beta_s-X_{ik}\beta_k)^2+\lambda |\beta_k| Repeat Step 2 Until Converge Reference从Lasso说起 LARS ESL]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F16%2FFundamentals%20of%20Value%20vs%20Growth%20Investing%20and%20an%20Explanation%20for%20the%20Value%20Trap%2F</url>
    <content type="text"><![CDATA[Fundamentals of Value vs. Growth Investing and an Explanation for the Value Trap1. E/P and Value TrapFor positive earnings, P_0 = \frac{Earnings_1}{r-g} \\ E_1/P_0 = r - g Given a HIgh E/P scenario: Value Trap: High growth with high risk, thus require high return Low growth with low risk 2. B/P and E/P B/P = \frac{B_0}{P_0}=\frac{B_0}{E_1}\frac{E_1}{P_0}=\frac{1}{ROE_1}(r-g) A lower ROE implies a higher B/P If a lower ROE implies higher future growth, a higher B/P is associated with a higher g. And it is also associated with a higher r. 3. Why expected growth might be inversely related to ROE?3.1 Conservative Accounting Earnings Recognition Principle: Under uncertainty, the recognition of earnings is deferred to the future until the uncertainty has been resolved. Delaying earnings recognition means more earnings in the future, that is, earnings growth. So, an expectation of future earnings that awaits “realization” is an expectation of earnings growth and, as that realization is tied to risk resolution, the expected growth is risky: it may not be realized.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Summarize Speculative Retail Trading and Asset Prices]]></title>
    <url>%2F2019%2F03%2F30%2FSummarize%20Speculative%20Retail%20Trading%20and%20Asset%20Prices%2F</url>
    <content type="text"><![CDATA[Introduction Following notes only summarize the parts that could be replicated in A-Shares. Introduce Retail trading proportion (RTP) measure Results Stocks with lottery features are heavily traded by retail investors and, thus, have high RTP High-RTP stocks tend to be overpriced Consistent with noise trader models in general Retail Trading ProxyData TAQ &amp; Broker RTP Measure The ratio of total month-t buy- and sell initiated small-trades dollar volume and the total stock trading dollar volume in the same month stock-level measure Trade size &lt; 5,000 RTP and Speculative Retail TradingSpeculation-Based Retail Clienteles Whether retail investors overweight speculative stocks in their portfolios. Each month, form portfolios sorted on IVol and LOTT LOTT Kumar (2009) Summary Stats (Monotonic) Regress RTP on characteristics (FM) Speculative Trading and Asset PricesRTP and Average Returns: Sorting Results RTP EW/VW Quantile Portfolio FF-4 Double-sorted with Size Fama-MacBeth Regression Estimates Stock return over the next month with RTP and controlled by characteristics and exposures. Test the RTP-return relation is stronger among stocks with speculative characteristics add RTP and IVol/LOTT interaction term. Reference paper link]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes on Three Pass Regression Filter]]></title>
    <url>%2F2019%2F03%2F29%2FNotes%20Three%20Pass%20Regression%20Filter%2F</url>
    <content type="text"><![CDATA[IntroductionIntuition Model Setting Latent factors drive the systematic variation of both the forecast target, y, and the matrix of predictors X. Problems Best prediction of y is infeasible due to latent factors. Benchmark Two Step Extract factors that are significant drivers of variation in X Use extracted factors to forecast Allow Sparsity among candidates The Three-Pass Regression FilterThe Estimator Notation $y_t, \ t=2,\cdots,T+1$ $X_{T,N}​$ TS-standardized predictors $Z_{T,L}$ proxies for latent factor These are variables, driven by the factors and are always available from the target and predictors themselves! Example: Guofu zhou Aligned Index Model Setting Target is decomposed into relevant and irrelevant factors. $\beta = (\beta_f^T,0)^T$ $F_t=(f_t,g_t)^T$ x_t = \phi_0+\phi F_t+\epsilon_t \\ y_{t+1} = \beta_0+\beta^TF_t+\eta_{t+1} \\ z_t = \lambda_0+\Lambda F_t+w_t For model assumptions and properties, please check the paper. 3PRF Procedure Step 1: N separate TS-regression (predictors on proxies) Estimate the sensitivity of the predictor to factors represented by the proxies Assumption Common components of proxies span the space of the target-relevant factors. First-stage coefficient estimates map the cross-sectional distribution of predictors to the latent factors Step 2: T separate cross-section regression (predictors on loadings) Use map from Step 1 to back out estimates of the factors at each point in time Step 1&amp;2 are similar to FM-2-stage Step 3: single forecasting TS-regression Empirical EvidenceForecasting Macroeconomic Aggregates Data 108 macroeconomic variables compiled by Stock and Watson (2012) Pseudo out-of-sample prediction Recursively estimate 3PRF and only use information up to t OOS $R^2$ Referencepaper link]]></content>
      <categories>
        <category>Notes</category>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Statistical Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize Investor Sentiment and the Cross-Section of Stock Returns]]></title>
    <url>%2F2019%2F03%2F29%2FSummarize%20Investor%20Sentiment%20and%20the%20Cross-Section%2F</url>
    <content type="text"><![CDATA[Theoretical Effects of SentimentSentiment Definition Propensity to speculate The lack of an earnings history combined with the presence of apparently unlimited growth opportunities allows unsophisticated investors to defend Optimism or pessimism in general: Arbitrage Risk risky and costly for young, small, unprofitable stocks Empirical Approach and DataEmpirical Approach Intuition Focus on systematic patterns of mispricing correction Control for generic impact of investor sentiment on all stocks ($a_1$) Control for the generic impact of characteristics across all time periods ($b_1​$) Focus on $b_2$ $H_0:b_2=0$, any nonzero effect is rational compensation for systematic risk Conditional characteristics model E_{t-1}(R_{it}) = \alpha+\alpha_1S_{t-1}+b_1^Tx_{i,t-1}+b_2^TS_{t-1}x_{i,t-1}Data Characteristics Size and Age Profitability Dividend Asset tangibility Growth/Distress Winsorizing Sentiment Index (Whole sample PCA) 6 proxies component Intuition Principal components analysis to isolate the common component Regress each of the six raw proxies on macro index and use residuals. Empirical TestsSorts Monthly return observation into a bin according to the decile rank that a characteristic takes at the beginning of that month and then according to the level of Residual Sentiment at the end of the previous calendar year. 10 EW decile Portfolio Results Subsequent returns tend to be higher, when sentiment is low. Most Positive - Negative &lt; 0 Opposite Sign Effect Age Volatility Lower Sentiment, less loss on long high risk portfolio Predictive Regression for L/S Portfolio Regress monthly L/S returns on 4-factor with sentiment Stambaugh 1999 bootstrapped SE R_t^{L/S} = c+dSentiment_{t-1}+\beta MKT_t+sSMB_t+hHML_t+mUMD_u+u_{it}= Summary When sentiment is high, future returns are relatively low for small firms, the youngest firms, firms with volatile stock returns, unprofitable firms, non-dividend-paying firms, high growth firms, and distressed firms. And vice-versa. Referencepaper link]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize Investor Sentiment Aligned A Powerful Predictor of Stock Returns]]></title>
    <url>%2F2019%2F03%2F28%2FSummarize%20Investment%20Sentiment%20Aligned%2F</url>
    <content type="text"><![CDATA[Conclusion PLS Improved version for BW 2006 Predictability is both statistically and economically significant Improve forecasting power for the cross-section of stock returns formed on industry, size, value, and momentum Predictability of investor sentiment seems to come from investors’ biased belief about future cash flows PLS Introduction Please check this note. Econometric Methodology for Aligned Sentiment IndexModel Setting One period return model, assume true sentiment is unobservable. E(R_{t+1})=\alpha+\beta S_t \\ R_{t+1} = \alpha+\beta S_t+\epsilon_{t+1} \\ E(e_{t+1}|S_t) = 0 Set of investor sentiment proxies (factor structure with common approximation error) X_{i,t} = \eta_{i,0}+\eta_{i,1}S_t+\eta_{i,2}E_t+e_{i,t} BW Drawbacks use PC1 of proxies as true sentiment Larger common approximation error, larger important role in estimation PLS as two step OLS For each proxy, run TS regression and get loadings x_{i,t-1} = \pi_{i,0}+\pi_iR_t+u_{i,t-1} \ \ t = 1,\cdots,T Run T CS regression get $S_t^{PLS}$ x_{i,t} = c_t+S_t^{PLS}\hat{\pi_i}+v_{i,t} \ \ i=1,\cdot,N Asymptotically converge Under 3PRF framework Sentiment as one Latent Factor Market return as both target and filter(latent proxies). DataData Used Take Excess return as market return Proxies Value-weighted average difference between the net asset values of closed-end stock mutual fund shares and their market prices Log of the raw turnover ratio de-trended by the past 5-year average Monthly number of initial public offerings Monthly average first-day returns of initial public offerings Log difference of the value-weighted average market-to-book ratios of dividend payers and non-payers Gross monthly equity issuance divided by gross monthly equity plus debt issuance Processing Each measure is standardized and regressed on macro variables. Sample Empirical ResultsForecasting the market Predictive Model R_{t+1}^m = \alpha+\beta S_t^k+\epsilon_{t+1} \ \ k=PLS,BW,EW Potential Problems and solution Spurious Regression Empirical p-values using a wild bootstrap Small-sample bias biased-adjusted regression coef method Forward Looking bias First stage uses information up to t Result 1 PLS is best EW and BW are similar Comparison with economic predictors Predictive Regression after controlling economics predictors 18 economic predictors Out-of-sample Recursively Estimating and predicting Measures OS R-square R_{OS}^2 = 1-\frac{\sum_{t=p}^{T-1}{(R_{t+1}^m-\hat{R}_{t+1}^m)^2}}{\sum_{t=p}^{T-1}{(R_{t+1}^m-\bar{R}_{t+1}^{m,historical})^2}} McCracken 2007 DM-test MSFE-adjusted of Clark and West 2007 Result Forecasting characteristic portfolios Industries technology, energy, and telecom industries are the most predictable Size, BM, Momentum Slopes on size are monotonic (Patton and Timmermann 2010) Economic ExplanationsCash Flow predictability High sentiment causes the over-valuation of aggregate stock market because of investors’overly optimistic belief about future aggregate cash flows Procedure OLS estimation R_{t+1}^m = \alpha+\beta^Tx_t+\psi^TZ_t+\epsilon_{t+1} Assume AR(1) on predictors (X and Z) \hat{\psi}_{i,x,t+1} = x_{i,t+1}-\hat{\rho}_{i,x,0}-\hat{\rho}_{i,x,1}x_{i,t} \\ \hat{\psi}_{i,z,t+1} = z_{i,t+1}-\hat{\rho}_{i,z,0}-\hat{\rho}_{i,z,1}z_{i,t} Reduced-bias estimation Nicholls and Pope (1988) Amihud, Hurvich, and Wang (2009) pseudo sample under no return predictability hypothesis w is draw from standard normal distribution $\epsilon$ is fitted residuals R_{t+1}^m=\bar{R}^{m,historical}+\hat{\epsilon}_{t+1}w_{t+1} \\ \tilde{x}_{i,t+1} = \hat{\rho}_{i,x,0}+\hat{\rho}_{i,x,1}\tilde{x}_{i,t}+\hat{\psi}_{i,x,t+1}w_{t+1} \\ \tilde{z}_{i,t+1} = \hat{\rho}_{i,z,0}+\hat{\rho}_{i,z,1}\tilde{z}_{i,t}+\hat{\psi}_{i,z,t+1}w_{t+1} Using the pseudo sample of observations to estimate regression ReferencePaper Link PLS Introduction%20analysis.pdf)]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes on PLS]]></title>
    <url>%2F2019%2F03%2F24%2FNotes%20PLS%20Introduction%2F</url>
    <content type="text"><![CDATA[PLS IntroductionStructural Equation Modeling Common Limitations A simple model structure The assumption that all variables can be considered as observable The conjecture that all variables are measured without error Structural Equation Modeling (SEM) Intuition distinguishes between the exogenous and endogenous latent variables Exogenous variables are not explained by the postulated model (i.e. act always as independent variables) Endogenous variables are explained by the relationships contained in the model. Framework Convert theoretical and derived concepts into unobservable (latent) variables Convert empirical concepts into indicators First equation: Indicators of exogenous variables and latent exogenous variables behind them x_1 = \lambda_{x,1,1}\xi_1+\delta_1 \\ x_2 = \lambda_{x,2,1}\xi_1+\delta_2 \\ x_3 = \lambda_{x,3,2}\xi_2+\delta_3 \\ \cdots \\ x_6 = \lambda_{x,6,3}\xi_3+\delta_6 x - indicators of the exogenous variables $\delta$ - measurement error of x $\xi$ - latent exogenous variables Second equation: Indicators of exogenous variables and latent exogenous variables behind them y1 = \lambda_{y,1,1}\eta_1+\epsilon_1 \\ \cdots \\ y_4 = \lambda_{y,4,2}\eta_2+\epsilon_4 Third equation: latent endogenous and exogenous variables with errors in equations \eta_1 = \gamma_{11}\xi_1+\zeta_1 \\ \eta_2 = \beta_{21}\eta_1+\gamma_{21}\xi_1+\gamma_{22}\xi_2+\gamma_{23}\xi_3+\zeta_2 Matrix Form x = \Lambda_x\Xi+\Delta \\ y = \Lambda_y\eta+\epsilon \\ \eta = \beta\eta+\Gamma \Xi+\zeta Two-step Estimation Basic Idea Step 1: Outside approximation (How latent variables can be decomposed into combination of indicators) Weights are determined in a manner similar to PCA Step 2: Inside structural Model Approximation Algorithm P are eigenvectors of $Y^TX$ Q are eigenvectors of $X^TY$ X = TP^T\ and \ Y=UQ^TNonlinear Iterative Partial Least Squares (NIPLS) Algorithm: find vector r and s \begin{equation} \begin{aligned} \max_{r,s} \quad & cov(Xr,Ys)^2\\ \textrm{s.t.} \quad & ||r|| = ||s|| = 1\\ \end{aligned} \end{equation} Start with random initialization of the y-space vector u and repeat: w = X^Tu/(u^Tu) \\ ||w|| \to 1 \\ t = Xw \\ c = Y^Tt/(t^Tt) \\ ||c|| \to 1 u = Yc Loadings p = X^Tt/(x^Tt) \\ q = Y^Tu/(u^Tu) Intuition PLS models try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. Assumptions Predictor Specification Drawbacks Underestimate the correlation between latent variables. Overestimate loadings Not scale invariant Advances in PLSCompared with PCA and CCA PCA \begin{equation} \begin{aligned} \max_{r} \quad & Var(Xr)\\ \textrm{s.t.} \quad & ||r|| = 1\\ \end{aligned} \end{equation} CCA \begin{equation} \begin{aligned} \max_{r,s} \quad & corr(Xr,Ys)^2\\ \textrm{s.t.} \quad & ||r|| = ||s|| = 1\\ \end{aligned} \end{equation} ReferencePLS Introduction PLS Review]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Statistical Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize Sentiment and the Performance of Technical Indicators]]></title>
    <url>%2F2019%2F03%2F22%2FSummarize%20Sentiment%20and%20the%20Performance%20of%20TI%2F</url>
    <content type="text"><![CDATA[Introduction and ConclusionIntuition Due to short-sale impediments, the overpricing in high-sentiment periodsis more prevalent than the underpricing in low-sentiment periods. Conclusion Technical indicators on Index perform better during periods of high sentiment Sentiment effect is relatively stronger for small stocks in comparison to large stocks Data and MethodologyData Sentiment Data Baker and Wurgler 2006/2007 market-based sentiment measure Orthogonal to macroeconomics conditions Technical Indicators Bloomberg 22 ‘BTST’ Index SP 500 Index SP Mid-cap Index SP Small-Cap 600 Index Dow Jones U.S. Large-Cap Index Dow Jones U.S. Mid-Cap Index Dow Jones U.S. Small-Cap Index Methodology Performance Measure raw return raw return control for VW market return Calmar Ratio Sharpe Ratio Examine the performance of the TI during periods of high sentiment with low sentiment Wilcoxon rank-sum tests with the null hypothesis that the reported mean value is not differentfrom zero. Daily trading is assumed over each one-year period Panel Regression Regress separately 4 measures on the level of sentiment each year include the index-fixed effects and indicator-fixed effects Group by Size Panel Regression with small-cap dummy ResultsTable for Methodology Section 2 and 3 Table for Methodology Section 4 and 5 Referencepaper link]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
        <tag>Technical Indicators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes on How to Win a Data Science Competition]]></title>
    <url>%2F2019%2F03%2F22%2FNotes%20How%20to%20Win%20a%20Data%20Science%20Competition%2F</url>
    <content type="text"><![CDATA[Feature preprocessing0. Missing Values EDA Treating values which do not present in train data Cat data Frequency Encoding Refill NaN Xgboost can handle that 1. Categorical and Ordinal Features Kaggle categorical feature Recap Target Encoding (2-Levels CV) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950n_folds = 20n_inner_folds = 10likelihood_encoded = pd.Series()likelihood_coding_map = &#123;&#125;oof_default_mean = train[target].mean() # global prior meankf = KFold(n_splits=n_folds, shuffle=True)oof_mean_cv = pd.DataFrame()split = 0for infold, oof in kf.split(train[feature]): print ('==============level 1 encoding..., fold %s ============' % split) inner_kf = KFold(n_splits=n_inner_folds, shuffle=True) inner_oof_default_mean = train.iloc[infold][target].mean() inner_split = 0 inner_oof_mean_cv = pd.DataFrame() likelihood_encoded_cv = pd.Series() for inner_infold, inner_oof in inner_kf.split(train.iloc[infold]): print ('==============level 2 encoding..., inner fold %s ============' % inner_split) # inner out of fold mean oof_mean = train.iloc[inner_infold].groupby(by=feature)[target].mean() # assign oof_mean to the infold likelihood_encoded_cv = likelihood_encoded_cv.append(train.iloc[infold].apply( lambda x : oof_mean[x[feature]] if x[feature] in oof_mean.index else inner_oof_default_mean, axis = 1)) inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer') inner_oof_mean_cv.fillna(inner_oof_default_mean, inplace=True) inner_split += 1 oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer') oof_mean_cv.fillna(value=oof_default_mean, inplace=True) split += 1 print ('============final mapping...===========') likelihood_encoded = likelihood_encoded.append(train.iloc[oof].apply( lambda x: np.mean(inner_oof_mean_cv.loc[x[feature]].values) if x[feature] in inner_oof_mean_cv.index else oof_default_mean, axis=1))######################################### map into test dataframetrain[feature] = likelihood_encodedlikelihood_coding_mapping = oof_mean_cv.mean(axis = 1)default_coding = oof_default_meanlikelihood_coding_map[feature] = (likelihood_coding_mapping, default_coding)mapping, default_mean = likelihood_coding_map[feature]test[feature] = test.apply(lambda x : mapping[x[feature]] if x[feature] in mapping else default_mean,axis = 1) Reference Winner Solution sklearn plugin BETA Target Encoding 2. Numeric Features Non-tree-based models hugely depend on scaling 3. DT and Coordinates DT Periodicity Day number Time Since DT Delta Coodinates Distance to interesting places Distance to centers of cluster 4. Text and Images Text BOW N-gram Embedding EDA0. Motivating Example Do not start with stacking! Promotion Prediction Magic feature Example Data Number of promos sent Number of promos used id Group by id and sorted by sent, then take difference Check intuitive Type age 336? 1. Understand how the data was generated Crucial for validation setting Ad Competition Example Problem (train/test data generated by different algo) Improve on validation does not improve LB LB metric much higher than train Findings Much more days in train, but less sample than test Training set is at least 1 impression Test set is all id Training is biased because lack of 0 impression 2. Anonymized data Guess Types Different types, different methods Guess Meaning Date Example 3. Visualization Explore Individual Feature Drop Var=0 features Single plot is misleading Hist, and take log check if missing values has been filled Index versus value plot check shuffle or not? check classification ability Explore feature relations scatter plot on two features with color specified by Label check areas covered by test samples Feature Group plt.matshow() clustering correlation cluster features Feature stats versus Feature Index sorted by values df.mean().sort_values().plot(style = ‘.’) 4. Other things to check Data Cleaning Constant/Duplicated features df.nunique() Samples Duplicated Rows have same label or not? why? if shuffled rolling target versus global target number of NAN per row Validation and Overfitting1. Validation Strategies Check duplicated rows/id for train-validation split Types Holdout K-fold Estimate mean and variance of the loss helpful to understand significant of improvement LOOCV 1from sklearn.model_selection import ShuffleSplit,Kfold,LeaveOneOut 2. Splitting Strategies Time-based splits By id Random Split 3. Common Validation Problems Validation Stage (big dispersion of scores on validation stage) Cause of different scores/parameters little data diverse and inconsistant Actions (more thorough validation) Average scores from more different KFolds Tube model on one split, evaluate score on the other Submission Stage Ensure same distribution LB shuffle Randomness (Two Sigma)/Little data/Different distribution sklearn validation Data Leakages1. Basic Leaks Leaks in time series Unexpected information Meta Data Information in ID Row Order Leaderboard Probing Metrics Optimization1. Regression MAE for outliers (Median) MSE, RMSE and R2 are the same Weighted version MSPE = \frac{100%}{N}\sum_{i=1}^N{(1-\hat{y}^i/y)^2} \\ MAPE = \frac{100%}{N}\sum_{i=1}^N{|1-\hat{y}^i/y|} Asymmetric RMSLE MSLE = \frac{1}{N}\sum_{i=1}^N(log(y_i+1)-log(\hat{y_i}+1))^22. Classification Metrics Logloss plog(ph) AUC Kappa Cohen's \ Kappa=1-\frac{1-acc}{1-p_{benchmark}}3. General Tricks Preprocess (MSPE/MAPE/MSLE) tricks transform target set sample weights in lib df.sample() to fit a new model (xgboost) MSPE as weighted MSE w_i = \frac{1/y_i^2}{\sum_{i=1}^N1/y^2_i} MAPE as weighted MAE w_i = \frac{1/y_i}{\sum_{i=1}^N1/yi} Postprocess(Acc/Kappa) Probability Calibration when not directly opt loss stacking for logloss Platt scaling (logistic on predictions) Isotonic regression For accuracy Fit any metric and tune threshold For auc pairwise loss in xgboost/lgbm For quadratric weighted Kappa Optimize MSE and find grid search threshold Soft Kappa Loss for xgboost Custom Loss Function Hubor Loss for MAE in xgboost Early stopping Stop when M2 is best Target Encodings Details1. Ways to target Likelihood = mean(target) Weight of Evidence = In(Goods/Bads) Count = sum(target) Diff=Goods-Bads 2. Regularization CV Loop Noise Smoothing 3. Extension to regression/multi-class More stats for regression Distribution Bins Bin numeric features and treat as cat data Mean Encoding for numeric features Start with raw numeric features in xgboost If lot of splits use splits to bin features and encode them Mean Encoding for cat interaction features Start with raw cat features in xgboost Count feature pairs interact in a tree Advanced Features1. Distanced-Based Features Example Mean encoding all variables need not to worry about scaling For every sample, find top 2000 neighbors using Bray-Curtis metric Bray-Curtis = \frac{|u-v|_1}{|u+v|_1} Generate Features Mean target of nearest 5,10,500 Mean distance to top 10 closest 2. Matrix Factorizations SVD/PCA Truncated SVD with sparse matrics Non-negative Matrix Factorization good for counts data Fit on the whole data set! 3. Feature Interactions Xgbfi Ensemble1. 1st Level tips Diversity based on algorithm Diversity based on input 2. Subsequent Level tips Make shallower Stacknet Details on Catboost1. Cat Data One-hot (one_hot_max_size) default Freq Labeling Statistics Labeling Consider combinations in a greedy way Hyper-parameter tuning1. Basic Models Tuning Methods Xgboost/Lightgbm min_child_weight! eta/num_round eta divided by alpha num_round multiplied by alpha use different seed check robustness if using early stopping, no need to tune number of iterations Random Forest/Extra Trees N_estimators the higher, the better max_depth start with 7 random_state as seed NN Opt Algo SGD+Momentum Adam/Adadelta/Adagrad faster training more overfitting Batch Size star with 32/64 Regularization Static dropconnect Start with 4096 first layer Fix 99% of the first layer to be dropped during training 2. Tips Don’t spend too much time! Average everything over seed over small deviations from optimal parameters Libs hyperopt Practical Guide1. Tips Data loading use hdf5/npy downcast to 32-bits chunks Start Read forums Begin with simple Train/Test Split EDA and Fastest Model (LGBM/RF) as a baseline check if validation is satble Add features in bulks create all features evaluate many at once (Xgboost) Debug full pipeline Only after little contribution from feature engineering/tuning single model, switch to fully CV stacking Links Course]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize The Long of it]]></title>
    <url>%2F2019%2F03%2F14%2FSummarize%20The%20Long%20of%20it%2F</url>
    <content type="text"><![CDATA[Empirical Settings Intuition: Spurious Regressor Problem in “The short of It” coefficient for sentiment index in predictive regression Repeating Simulation and Joint Distribution 200m times Simulating sentiment index by generating first-order autoregressive process with normal innovations and autocorrelation equal to adjusted sentiment index acf(1) Joint Comparisons of t-stats Notation $\bar{t}_i^S:i-th \ highest \ t-stat \ among \ 11 \ anomalies \ under \ Sentiment $ $\bar{t}_i^X:i-th \ highest \ t-stat \ among \ 11 \ anomalies \ under \ Simulation$ $t_C^S​$ t-stat for combination of 11 anomalies under Sentiment $t_C^X$ t-stat for combination of 11 anomalies under Simulation L/S Hypothesis 1 Test $b&gt;0​$, which equals to $\bar{t}_i^X\geq\bar{t_i}^S​$and $t_C^X\geq t_C^S​$ Short Leg Hypothesis 2 Test $b&lt;0​$, which equals to $\underline{t}_i^X\leq\underline{t_i}^S​$and $t_C^X\leq t_C^S​$ Long Leg Hypothesis 3 Test $b=0$, which equals to $|t_i^X|\leq |t_i|^S$and $|t_C|^X\leq |t_C|^S$ Table 1 Referencepaper link]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize The short of it]]></title>
    <url>%2F2019%2F03%2F14%2FSummarize%20The%20Short%20of%20it%2F</url>
    <content type="text"><![CDATA[Main Results Investor sentiment contains a market-wide component Influence prices on securities in the same direction at the same time Short selling as the major obstacle to eliminating sentiment-driven mispricing. Hypothesis 1 Anomalies should be stronger following high sentiment. (Short-Sell problem) Intuition: During low-sentiment periods, most optimistic views are from rational investors. 70% of adjusted profits from a L/S occur in months following levels of investor sentiment above its median value. Hypothesis 2 Profits from the short leg should be greater when sentiment is high, and the stocks in the short leg are relatively overpriced compared to the stocks in the long leg. 78% of the adjusted profits from shorting that leg occur in months following high sentiment. Hypothesis 3 Sentiment does not greatly affect returns on the long-leg portfolio. None of the 11 long legs exhibits a significantdifference between high- and low-sentiment periods Data BW 2006 sentiment Index 11 anomalies VW within decile Empirical ResultsSentiment and returns High-Sentiment: BW in the previous month &gt; median Table 3 t-stat for FF3-adjusted return Long leg HIgh sentiment Low sentiment H-L Short leg L/S Table 8 Predictive Regression FF3+BW Index on Return/Spreads FF-3+Sentiment+Macro Control Referencepaper link]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize Factor Momentum Everywhere]]></title>
    <url>%2F2019%2F03%2F05%2FSummarize%20Factor%20Momentum%20Everywhere%2F</url>
    <content type="text"><![CDATA[Summary Factor Momentum exists. FM Portfolio works. 1-1 is still Momentum. Global works. Data 65 characteristic-based factor portfolio since 1960 preprocessing winsorize 1% 2*3 within group VW monthly NYSE median mkt cap cutoff factor 30/40/30 [(LH+SH)-(LL+SL)]/2 Factors Value factor exposure size investment Profitbability Risk Liqudity Factor Momentum1. Factor Persistence AR(1) coef 50/65 &gt; market 2. Time-Series Factor Momentum Similar to 2012 AQR’s TS Momentum Volatility Scaling lookback &lt; 12, 3 year vol otherwise, 10 year vol one month TSFM return = Factor Return * signal signal is total return given lookback window scaled by vol Cap signal between -2 and 2 Assesment Sharpe EW Factor alpha FF5 alpha Overall TSFMTSFM_{W} = TSFM_{W}^L-TSFM_{W}^S \\ TSFM_{W,t}^L = \frac{\sum_i{1^Lf_{i,W,t+1}^{TSFM}}}{\sum_i1^Ls_{i,W,t}} 3. Compared with Other Mom Exhibit 7 Correlation 1-1 CSFM/TSFM vs STR -0.8 (Not stock -level reversal!) 2-12 0.75 (Momentum!) Exhibit 8 TSFM more effective UMD only explains 2-12 TSFM CSFM more correlated with UMD and INDMOM]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
</search>
