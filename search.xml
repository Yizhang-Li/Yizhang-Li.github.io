<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Notes on PLS]]></title>
    <url>%2F2019%2F03%2F25%2FNotes%20PLS%20Introduction%2F</url>
    <content type="text"><![CDATA[PLS IntroductionStructural Equation Modeling Common Limitations A simple model structure The assumption that all variables can be considered as observable The conjecture that all variables are measured without error Structural Equation Modeling (SEM) Intuition distinguishes between the exogenous and endogenous latent variables Exogenous variables are not explained by the postulated model (i.e. act always as independent variables) Endogenous variables are explained by the relationships contained in the model. Framework Convert theoretical and derived concepts into unobservable (latent) variables Convert empirical concepts into indicators First equation: Indicators of exogenous variables and latent exogenous variables behind them x_1 = \lambda_{x,1,1}\xi_1+\delta_1 \\ x_2 = \lambda_{x,2,1}\xi_1+\delta_2 \\ x_3 = \lambda_{x,3,2}\xi_2+\delta_3 \\ \cdots \\ x_6 = \lambda_{x,6,3}\xi_3+\delta_6 x - indicators of the exogenous variables $\delta$ - measurement error of x $\xi$ - latent exogenous variables Second equation: Indicators of exogenous variables and latent exogenous variables behind them y1 = \lambda_{y,1,1}\eta_1+\epsilon_1 \\ \cdots \\ y_4 = \lambda_{y,4,2}\eta_2+\epsilon_4 Third equation: latent endogenous and exogenous variables with errors in equations \eta_1 = \gamma_{11}\xi_1+\zeta_1 \\ \eta_2 = \beta_{21}\eta_1+\gamma_{21}\xi_1+\gamma_{22}\xi_2+\gamma_{23}\xi_3+\zeta_2 Matrix Form x = \Lambda_x\Xi+\Delta \\ y = \Lambda_y\eta+\epsilon \\ \eta = \beta\eta+\Gamma \Xi+\zeta Two-step Estimation Basic Idea Step 1: Outside approximation (How latent variables can be decomposed into combination of indicators) Weights are determined in a manner similar to PCA Step 2: Inside structural Model Approximation Algorithm P are eigenvectors of $Y^TX$ Q are eigenvectors of $X^TY$ X = TP^T\ and \ Y=UQ^TIntuition PLS models try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. Assumptions Predictor Specification Drawbacks Underestimate the correlation between latent variables. Overestimate loadings Not scale invariant ReferencePLS Introduction]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Statistical Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize Sentiment and the Performance of Technical Indicators]]></title>
    <url>%2F2019%2F03%2F22%2FSummarize%20Sentiment%20and%20the%20Performance%20of%20TI%2F</url>
    <content type="text"><![CDATA[Introduction and ConclusionIntuition Due to short-sale impediments, the overpricing in high-sentiment periodsis more prevalent than the underpricing in low-sentiment periods. Conclusion Technical indicators on Index perform better during periods of high sentiment Sentiment effect is relatively stronger for small stocks in comparison to large stocks Data and MethodologyData Sentiment Data Baker and Wurgler 2006/2007 market-based sentiment measure Orthogonal to macroeconomics conditions Technical Indicators Bloomberg 22 ‘BTST’ Index SP 500 Index SP Mid-cap Index SP Small-Cap 600 Index Dow Jones U.S. Large-Cap Index Dow Jones U.S. Mid-Cap Index Dow Jones U.S. Small-Cap Index Methodology Performance Measure raw return raw return control for VW market return Calmar Ratio Sharpe Ratio Examine the performance of the TI during periods of high sentiment with low sentiment Wilcoxon rank-sum tests with the null hypothesis that the reported mean value is not differentfrom zero. Daily trading is assumed over each one-year period Panel Regression Regress separately 4 measures on the level of sentiment each year include the index-fixed effects and indicator-fixed effects Group by Size Panel Regression with small-cap dummy ResultsTable for Methodology Section 2 and 3 Table for Methodology Section 4 and 5 Referencepaper link]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
        <tag>Technical Indicators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes on How to Win a Data Science Competition]]></title>
    <url>%2F2019%2F03%2F22%2FNotes%20How%20to%20Win%20a%20Data%20Science%20Competition%2F</url>
    <content type="text"><![CDATA[Feature preprocessing0. Missing Values EDA Treating values which do not present in train data Cat data Frequency Encoding Refill NaN Xgboost can handle that 1. Categorical and Ordinal Features Kaggle categorical feature Recap Target Encoding (2-Levels CV) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950n_folds = 20n_inner_folds = 10likelihood_encoded = pd.Series()likelihood_coding_map = &#123;&#125;oof_default_mean = train[target].mean() # global prior meankf = KFold(n_splits=n_folds, shuffle=True)oof_mean_cv = pd.DataFrame()split = 0for infold, oof in kf.split(train[feature]): print ('==============level 1 encoding..., fold %s ============' % split) inner_kf = KFold(n_splits=n_inner_folds, shuffle=True) inner_oof_default_mean = train.iloc[infold][target].mean() inner_split = 0 inner_oof_mean_cv = pd.DataFrame() likelihood_encoded_cv = pd.Series() for inner_infold, inner_oof in inner_kf.split(train.iloc[infold]): print ('==============level 2 encoding..., inner fold %s ============' % inner_split) # inner out of fold mean oof_mean = train.iloc[inner_infold].groupby(by=feature)[target].mean() # assign oof_mean to the infold likelihood_encoded_cv = likelihood_encoded_cv.append(train.iloc[infold].apply( lambda x : oof_mean[x[feature]] if x[feature] in oof_mean.index else inner_oof_default_mean, axis = 1)) inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer') inner_oof_mean_cv.fillna(inner_oof_default_mean, inplace=True) inner_split += 1 oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer') oof_mean_cv.fillna(value=oof_default_mean, inplace=True) split += 1 print ('============final mapping...===========') likelihood_encoded = likelihood_encoded.append(train.iloc[oof].apply( lambda x: np.mean(inner_oof_mean_cv.loc[x[feature]].values) if x[feature] in inner_oof_mean_cv.index else oof_default_mean, axis=1))######################################### map into test dataframetrain[feature] = likelihood_encodedlikelihood_coding_mapping = oof_mean_cv.mean(axis = 1)default_coding = oof_default_meanlikelihood_coding_map[feature] = (likelihood_coding_mapping, default_coding)mapping, default_mean = likelihood_coding_map[feature]test[feature] = test.apply(lambda x : mapping[x[feature]] if x[feature] in mapping else default_mean,axis = 1) Reference Winner Solution sklearn plugin BETA Target Encoding 2. Numeric Features Non-tree-based models hugely depend on scaling 3. DT and Coordinates DT Periodicity Day number Time Since DT Delta Coodinates Distance to interesting places Distance to centers of cluster 4. Text and Images Text BOW N-gram Embedding EDA0. Motivating Example Do not start with stacking! Promotion Prediction Magic feature Example Data Number of promos sent Number of promos used id Group by id and sorted by sent, then take difference Check intuitive Type age 336? 1. Understand how the data was generated Crucial for validation setting Ad Competition Example Problem (train/test data generated by different algo) Improve on validation does not improve LB LB metric much higher than train Findings Much more days in train, but less sample than test Training set is at least 1 impression Test set is all id Training is biased because lack of 0 impression 2. Anonymized data Guess Types Different types, different methods Guess Meaning Date Example 3. Visualization Explore Individual Feature Drop Var=0 features Single plot is misleading Hist, and take log check if missing values has been filled Index versus value plot check shuffle or not? check classification ability Explore feature relations scatter plot on two features with color specified by Label check areas covered by test samples Feature Group plt.matshow() clustering correlation cluster features Feature stats versus Feature Index sorted by values df.mean().sort_values().plot(style = ‘.’) 4. Other things to check Data Cleaning Constant/Duplicated features df.nunique() Samples Duplicated Rows have same label or not? why? if shuffled rolling target versus global target number of NAN per row Validation and Overfitting1. Validation Strategies Check duplicated rows/id for train-validation split Types Holdout K-fold Estimate mean and variance of the loss helpful to understand significant of improvement LOOCV 1from sklearn.model_selection import ShuffleSplit,Kfold,LeaveOneOut 2. Splitting Strategies Time-based splits By id Random Split 3. Common Validation Problems Validation Stage (big dispersion of scores on validation stage) Cause of different scores/parameters little data diverse and inconsistant Actions (more thorough validation) Average scores from more different KFolds Tube model on one split, evaluate score on the other Submission Stage Ensure same distribution LB shuffle Randomness (Two Sigma)/Little data/Different distribution sklearn validation Data Leakages1. Basic Leaks Leaks in time series Unexpected information Meta Data Information in ID Row Order Leaderboard Probing Metrics Optimization1. Regression MAE for outliers (Median) MSE, RMSE and R2 are the same Weighted version MSPE = \frac{100%}{N}\sum_{i=1}^N{(1-\hat{y}^i/y)^2} \\ MAPE = \frac{100%}{N}\sum_{i=1}^N{|1-\hat{y}^i/y|} Asymmetric RMSLE MSLE = \frac{1}{N}\sum_{i=1}^N(log(y_i+1)-log(\hat{y_i}+1))^22. Classification Metrics Logloss plog(ph) AUC Kappa Cohen's \ Kappa=1-\frac{1-acc}{1-p_{benchmark}}3. General Tricks Preprocess (MSPE/MAPE/MSLE) tricks transform target set sample weights in lib df.sample() to fit a new model (xgboost) MSPE as weighted MSE w_i = \frac{1/y_i^2}{\sum_{i=1}^N1/y^2_i} MAPE as weighted MAE w_i = \frac{1/y_i}{\sum_{i=1}^N1/yi} Postprocess(Acc/Kappa) Probability Calibration when not directly opt loss stacking for logloss Platt scaling (logistic on predictions) Isotonic regression For accuracy Fit any metric and tune threshold For auc pairwise loss in xgboost/lgbm For quadratric weighted Kappa Optimize MSE and find grid search threshold Soft Kappa Loss for xgboost Custom Loss Function Hubor Loss for MAE in xgboost Early stopping Stop when M2 is best Target Encodings Details1. Ways to target Likelihood = mean(target) Weight of Evidence = In(Goods/Bads) Count = sum(target) Diff=Goods-Bads 2. Regularization CV Loop Noise Smoothing 3. Extension to regression/multi-class More stats for regression Distribution Bins Bin numeric features and treat as cat data Mean Encoding for numeric features Start with raw numeric features in xgboost If lot of splits use splits to bin features and encode them Mean Encoding for cat interaction features Start with raw cat features in xgboost Count feature pairs interact in a tree Advanced Features1. Distanced-Based Features Example Mean encoding all variables need not to worry about scaling For every sample, find top 2000 neighbors using Bray-Curtis metric Bray-Curtis = \frac{|u-v|_1}{|u+v|_1} Generate Features Mean target of nearest 5,10,500 Mean distance to top 10 closest 2. Matrix Factorizations SVD/PCA Truncated SVD with sparse matrics Non-negative Matrix Factorization good for counts data Fit on the whole data set! 3. Feature Interactions Xgbfi Ensemble1. 1st Level tips Diversity based on algorithm Diversity based on input 2. Subsequent Level tips Make shallower Stacknet Details on Catboost1. Cat Data One-hot (one_hot_max_size) default Freq Labeling Statistics Labeling Consider combinations in a greedy way Hyper-parameter tuning1. Basic Models Tuning Methods Xgboost/Lightgbm min_child_weight! eta/num_round eta divided by alpha num_round multiplied by alpha use different seed check robustness if using early stopping, no need to tune number of iterations Random Forest/Extra Trees N_estimators the higher, the better max_depth start with 7 random_state as seed NN Opt Algo SGD+Momentum Adam/Adadelta/Adagrad faster training more overfitting Batch Size star with 32/64 Regularization Static dropconnect Start with 4096 first layer Fix 99% of the first layer to be dropped during training 2. Tips Don’t spend too much time! Average everything over seed over small deviations from optimal parameters Libs hyperopt Practical Guide1. Tips Data loading use hdf5/npy downcast to 32-bits chunks Start Read forums Begin with simple Train/Test Split EDA and Fastest Model (LGBM/RF) as a baseline check if validation is satble Add features in bulks create all features evaluate many at once (Xgboost) Debug full pipeline Only after little contribution from feature engineering/tuning single model, switch to fully CV stacking Links Course]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Notes</tag>
        <tag>Machine Learning</tag>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize The Long of it]]></title>
    <url>%2F2019%2F03%2F15%2FSummarize%20The%20Long%20of%20it%2F</url>
    <content type="text"><![CDATA[Empirical Settings Intuition: Spurious Regressor Problem in “The short of It” coefficient for sentiment index in predictive regression Repeating Simulation and Joint Distribution 200m times Simulating sentiment index by generating first-order autoregressive process with normal innovations and autocorrelation equal to adjusted sentiment index acf(1) Joint Comparisons of t-stats Notation $\bar{t}_i^S:i-th \ highest \ t-stat \ among \ 11 \ anomalies \ under \ Sentiment $ $\bar{t}_i^X:i-th \ highest \ t-stat \ among \ 11 \ anomalies \ under \ Simulation$ $t_C^S​$ t-stat for combination of 11 anomalies under Sentiment $t_C^X$ t-stat for combination of 11 anomalies under Simulation L/S Hypothesis 1 Test $b&gt;0​$, which equals to $\bar{t}_i^X\geq\bar{t_i}^S​$and $t_C^X\geq t_C^S​$ Short Leg Hypothesis 2 Test $b&lt;0​$, which equals to $\underline{t}_i^X\leq\underline{t_i}^S​$and $t_C^X\leq t_C^S​$ Long Leg Hypothesis 3 Test $b=0$, which equals to $|t_i^X|\leq |t_i|^S$and $|t_C|^X\leq |t_C|^S$ Table 1]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize The short of it]]></title>
    <url>%2F2019%2F03%2F14%2FSummarize%20The%20Short%20of%20it%2F</url>
    <content type="text"><![CDATA[Main Results Investor sentiment contains a market-wide component Influence prices on securities in the same direction at the same time Short selling as the major obstacle to eliminating sentiment-driven mispricing. Hypothesis 1 Anomalies should be stronger following high sentiment. (Short-Sell problem) Intuition: During low-sentiment periods, most optimistic views are from rational investors. 70% of adjusted profits from a L/S occur in months following levels of investor sentiment above its median value. Hypothesis 2 Profits from the short leg should be greater when sentiment is high, and the stocks in the short leg are relatively overpriced compared to the stocks in the long leg. 78% of the adjusted profits from shorting that leg occur in months following high sentiment. Hypothesis 3 Sentiment does not greatly affect returns on the long-leg portfolio. None of the 11 long legs exhibits a significantdifference between high- and low-sentiment periods Data BW 2006 sentiment Index 11 anomalies VW within decile Empirical ResultsSentiment and returns High-Sentiment: BW in the previous month &gt; median Table 3 t-stat for FF3-adjusted return Long leg HIgh sentiment Low sentiment H-L Short leg L/S Table 8 Predictive Regression FF3+BW Index on Return/Spreads FF-3+Sentiment+Macro Control]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summarize Factor Momentum Everywhere]]></title>
    <url>%2F2019%2F03%2F05%2FSummarize%20Factor%20Momentum%20Everywhere%2F</url>
    <content type="text"><![CDATA[Summary Factor Momentum exists. FM Portfolio works. 1-1 is still Momentum. Global works. Data 65 characteristic-based factor portfolio since 1960 preprocessing winsorize 1% 2*3 within group VW monthly NYSE median mkt cap cutoff factor 30/40/30 [(LH+SH)-(LL+SL)]/2 Factors Value factor exposure size investment Profitbability Risk Liqudity Factor Momentum1. Factor Persistence AR(1) coef 50/65 &gt; market 2. Time-Series Factor Momentum Similar to 2012 AQR’s TS Momentum Volatility Scaling lookback &lt; 12, 3 year vol otherwise, 10 year vol one month TSFM return = Factor Return * signal signal is total return given lookback window scaled by vol Cap signal between -2 and 2 Assesment Sharpe EW Factor alpha FF5 alpha Overall TSFMTSFM_{W} = TSFM_{W}^L-TSFM_{W}^S \\ TSFM_{W,t}^L = \frac{\sum_i{1^Lf_{i,W,t+1}^{TSFM}}}{\sum_i1^Ls_{i,W,t}} 3. Compared with Other Mom Exhibit 7 Correlation 1-1 CSFM/TSFM vs STR -0.8 (Not stock -level reversal!) 2-12 0.75 (Momentum!) Exhibit 8 TSFM more effective UMD only explains 2-12 TSFM CSFM more correlated with UMD and INDMOM]]></content>
      <categories>
        <category>Paper Review</category>
      </categories>
      <tags>
        <tag>Empirical Asset Pricing</tag>
        <tag>Sentiment</tag>
      </tags>
  </entry>
</search>
